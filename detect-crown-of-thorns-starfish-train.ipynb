{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport ast\nimport yaml\nimport torch\nimport shutil\nimport random\nimport numpy as np\nimport pandas as pd\nfrom joblib import Parallel, delayed\nfrom sklearn.model_selection import GroupKFold\n\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sys\nsys.path.append('../input/d/cowfrica/yolov5')\nsys.path.append('../input/hyperparameters-for-yolov5')\nsys.path.append('../input/tensorflow-great-barrier-reef')\n\nimport utils\ntqdm.pandas()\n\n%pip install -q wandb\n%pip install wandb --upgrade\nimport wandb\nwandb.login(key=\"f04c0b8d3b383666c2518b204435adcb3f9532e9\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-26T21:18:07.943813Z","iopub.execute_input":"2021-12-26T21:18:07.944985Z","iopub.status.idle":"2021-12-26T21:18:28.995911Z","shell.execute_reply.started":"2021-12-26T21:18:07.94492Z","shell.execute_reply":"2021-12-26T21:18:28.994842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Configuration","metadata":{}},{"cell_type":"code","source":"def random_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        \nrandom_seed(1702)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T21:21:00.9859Z","iopub.execute_input":"2021-12-26T21:21:00.98636Z","iopub.status.idle":"2021-12-26T21:21:00.999228Z","shell.execute_reply.started":"2021-12-26T21:21:00.986317Z","shell.execute_reply":"2021-12-26T21:21:00.997952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FOLD = 4 \nIMAGE_WIDTH = 1280\nIMAGE_HEIGHT = 720\nREMOVE_NOBBOX = True \nNUM_BACKGROUND_IMG = 400\n\nCWD = '/kaggle/working/'\nIMAGE_DIR = '/kaggle/images' \nLABEL_DIR = '/kaggle/labels' \nROOT_DIR = '/kaggle/input/tensorflow-great-barrier-reef/'","metadata":{"execution":{"iopub.status.busy":"2021-12-26T21:21:01.326304Z","iopub.execute_input":"2021-12-26T21:21:01.326617Z","iopub.status.idle":"2021-12-26T21:21:01.332414Z","shell.execute_reply.started":"2021-12-26T21:21:01.326581Z","shell.execute_reply":"2021-12-26T21:21:01.331253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p {IMAGE_DIR}\n!mkdir -p {LABEL_DIR}","metadata":{"execution":{"iopub.status.busy":"2021-12-26T21:21:01.725257Z","iopub.execute_input":"2021-12-26T21:21:01.725567Z","iopub.status.idle":"2021-12-26T21:21:03.25752Z","shell.execute_reply.started":"2021-12-26T21:21:01.725535Z","shell.execute_reply":"2021-12-26T21:21:03.256091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Helpers\n\nI don't write any code in this section.","metadata":{}},{"cell_type":"code","source":"def coco2yolo(image_width, image_height, bboxes):\n    bboxes = bboxes.copy().astype(float) \n    \n    # normalize\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]] / image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]] / image_height\n    \n    # gets xmid and ymid \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]] / 2\n    \n    return bboxes\n\ndef get_bbox(annots):\n    # converts from dictionary to list \n    # formart after converting: [x, y, width, height]\n    \n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes","metadata":{"execution":{"iopub.status.busy":"2021-12-26T21:21:03.2598Z","iopub.execute_input":"2021-12-26T21:21:03.260409Z","iopub.status.idle":"2021-12-26T21:21:03.268834Z","shell.execute_reply.started":"2021-12-26T21:21:03.260363Z","shell.execute_reply":"2021-12-26T21:21:03.26805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Dataset","metadata":{}},{"cell_type":"code","source":"def get_path(row):\n    row['old_image_path'] = f'{ROOT_DIR}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n    row['image_path'] = f'{IMAGE_DIR}/video_{row.video_id}_{row.video_frame}.jpg'\n    row['label_path'] = f'{LABEL_DIR}/video_{row.video_id}_{row.video_frame}.txt'\n    \n    return row\n\ndf = pd.read_csv(f'{ROOT_DIR}/train.csv')\ndf = df.progress_apply(get_path, axis=1)\ndf['annotations'] = df['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndf['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndf['bboxes'] = df.annotations.progress_apply(get_bbox)\n\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T21:21:03.269868Z","iopub.execute_input":"2021-12-26T21:21:03.270422Z","iopub.status.idle":"2021-12-26T21:21:13.602821Z","shell.execute_reply.started":"2021-12-26T21:21:03.270384Z","shell.execute_reply":"2021-12-26T21:21:13.601358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gets background images\ndf_empty_bbox = df[df[\"num_bbox\"] == 0]\ndf_empty_bbox = df_empty_bbox.sample(frac=1).reset_index(drop=True).iloc[:NUM_BACKGROUND_IMG,]\n\n# gets images with objects\ndf = df[df[\"num_bbox\"] > 0]\n\n# concats background images and image with objects\ndf = pd.concat([df, df_empty_bbox], ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# copy from the original path to kaggle/working \n# because the models requires folder that can be written data on\n\ndef make_copy(path):\n    data = path.split('/')\n    filename = data[-1]\n    video_id = data[-2]\n    new_path = os.path.join(IMAGE_DIR, f'{video_id}_{filename}')\n    shutil.copy(path, new_path)\n    return\n\n# using Parrallel for faster copying \nimage_paths = df.old_image_path.tolist()\n_ = Parallel(n_jobs=-1, backend='threading')(delayed(make_copy)(path) for path in tqdm(image_paths))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# perform KFold\n\nkf = GroupKFold(n_splits = 5)\ndf = df.reset_index(drop=True)\ndf['fold'] = -1\n\nfor fold, (_, val_idx) in enumerate(kf.split(df, y=df.video_id.tolist(), groups=df.sequence)):\n    df.loc[val_idx, 'fold'] = fold\n\ndf.fold.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = df[df.fold != FOLD]\nvalid_df = df[df.fold == FOLD]\n\ntrain_df.shape[0], valid_df.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Files required for YOLOv5\n\nMore details about the requirements can be found [here](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data).","metadata":{}},{"cell_type":"code","source":"# dataset.yaml for YOLOv5\n\nwith open(os.path.join(CWD, 'train.txt'), 'w') as f:\n    for path in train_df.image_path.tolist():\n        f.write(path + '\\n')\n            \nwith open(os.path.join(CWD, 'val.txt'), 'w') as f:\n    for path in valid_df.image_path.tolist():\n        f.write(path + '\\n')\n\ndata = dict(\n    path  = CWD,\n    train =  os.path.join(CWD, 'train.txt'),\n    val   =  os.path.join(CWD, 'val.txt' ),\n    nc    = 1,\n    names = ['cots'],\n    )\n\nwith open(os.path.join(CWD, 'starfish.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(os.path.join(CWD, 'starfish.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# labels for YOLOv5\n\nfor row_idx in tqdm(range(df.shape[0])):\n    row = df.iloc[row_idx]\n    bboxes_coco = np.array(row.bboxes).astype(np.float32).copy()\n    num_bbox = row.num_bbox\n    names = ['cots'] * num_bbox\n    labels = [0] * num_bbox\n\n    with open(row.label_path, 'w') as f:\n        if num_bbox < 1:\n            annot = ''\n            f.write(annot)\n            continue\n            \n        bboxes_yolo  = coco2yolo(IMAGE_WIDTH, IMAGE_HEIGHT, bboxes_coco)\n        bboxes_yolo  = np.clip(bboxes_yolo, 0, 1)\n        \n        for bbox_idx in range(len(bboxes_yolo)):\n            label = [str(labels[bbox_idx])]\n            bboxes = list(bboxes_yolo[bbox_idx].astype(str))\n            new_line = (['\\n'] if num_bbox != (bbox_idx + 1) else [''])\n            \n            annot =  label + bboxes + new_line\n            annot = ' '.join(annot)\n            annot = annot.strip(' ')\n            \n            f.write(annot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Training","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!rm -r /kaggle/working/yolov5\n!cp -r ../input/d/cowfrica/yolov5 /kaggle/working/yolov5\n%cd yolov5\n\n!python train.py --img 1280\\\n--batch 10\\\n--epochs 20\\\n--data /kaggle/working/starfish.yaml\\\n--weights yolov5m.pt --workers 0\\\n--adam\\\n--save-period 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}